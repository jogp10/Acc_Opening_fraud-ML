{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Intelligence for Business\n",
    "\n",
    "This Jupyter notebook performs Exploratory Data Analysis (EDA) on one of the six synthetic tabular datasets in the Bank Account Fraud (BAF) suite of datasets. The BAF datasets were published at NeurIPS 2022 and are intended to provide a realistic, complete, and robust test bed to evaluate novel and existing methods in machine learning (ML) and fair ML."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Description"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of this notebook is to provide an overview over the dataset and prepare it to be used for training and evaluating ML models. The notebook is structured as follows:\n",
    "- [1. Load the dataset](#1.-Load-the-dataset)\n",
    "- [2. Explore the dataset](#2.-Explore-the-dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The BAF suite of datasets comprises a total of 6 different synthetic bank account fraud tabular datasets. The datasets are realistic, based on a present-day real-world dataset for fraud detection, and each dataset has distinct controlled types of bias. Additionally, the datasets have an imbalanced setting with an extremely low prevalence of positive class, contain temporal data and observed distribution shifts, and have privacy-preserving features to protect the identity of potential applicants.\n",
    "\n",
    "In this notebook, we will be exploring one of the datasets in the BAF suite, the Base.csv dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "from tabulate import tabulate\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc, f1_score, accuracy_score, precision_score, recall_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Understanding and Exploration"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and describing the dataset\n",
    "We start by loading the dataset into a Pandas DataFrame and displaying its first few rows using the head() function. We then display some basic statistics of the dataset using the describe() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Read in the data\n",
    "    df = pd.read_csv('../dataset/Base.csv', header=0)\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: File not found.\")\n",
    "except pd.errors.EmptyDataError:\n",
    "    print(\"Error: Empty DataFrame.\")\n",
    "except pd.errors.ParserError:\n",
    "    print(\"Error: Parsing error occurred.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {str(e)}\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boxplot\n",
    "We then use the Seaborn library to create boxplots of the numerical columns in the dataset. Boxplots are used to visualize the distribution and outliers of each numerical column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the list of numerical columns\n",
    "num_cols = df.select_dtypes(include=['float', 'int']).columns.tolist()\n",
    "\n",
    "# create a grid of subplots using seaborn\n",
    "n_cols = 3  # number of columns in the grid\n",
    "n_rows = (len(num_cols) + n_cols - 1) // n_cols  # number of rows in the grid\n",
    "fig, axes = plt.subplots(nrows=n_rows, ncols=n_cols, figsize=(25, 5*n_rows))\n",
    "\n",
    "# loop through the columns and create a boxplot for each one\n",
    "for i, col in enumerate(num_cols):\n",
    "    row_idx = i // n_cols  # row index for this subplot\n",
    "    col_idx = i % n_cols  # column index for this subplot\n",
    "    ax = sns.boxplot(data=df[col], ax=axes[row_idx, col_idx])\n",
    "    ax.set_title(col)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the analysis of the boxplots, we have identified a column that contains only a single value, with an equal value for all data inputs. Since this column does not provide any meaningful variation or information, it is recommended to remove it from the dataset before proceeding with further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_to_remove = 'device_fraud_count'\n",
    "\n",
    "# drop the column from the dataframe\n",
    "df = df.drop(column_to_remove, axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histograms\n",
    "We can also create histograms of the numerical columns to see the distribution of each feature. This can help us identify any features that may need to be transformed to achieve a normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.hist(bins=20, figsize=(25, 20))\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see some columns follow a normal distribution. Those columns are zip_count_4w, velocity_6h, velocity_24h, date_of_birth_distinct_emails_4w, credit_risk_score."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Plot\n",
    "Finally, we can create a count plot to visualize the distribution of the target variable (fraud). This can help us identify the class imbalance in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a count plot of the target variable\n",
    "sns.countplot(x='fraud_bool', data=df)\n",
    "plt.title('Target Variable Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Division of Variables\n",
    "The variables have been grouped as follows:\n",
    "- Target variable: Variable of interest in the project\n",
    "- Continuous variables: These variables represent quantitative measurements\n",
    "- Categorical variables: These variables represent a finite set of possible values\n",
    "- Binary variables: These variables have two distinct values, normally representing a yes/no condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous_cols = ['income', 'name_email_similarity', 'prev_address_months_count', 'current_address_months_count', 'customer_age', 'days_since_request', 'intended_balcon_amount', 'zip_count_4w', 'velocity_6h', 'velocity_24h', 'velocity_4w', 'bank_branch_count_8w', 'date_of_birth_distinct_emails_4w', 'credit_risk_score', 'bank_months_count', 'proposed_credit_limit', 'session_length_in_minutes', 'device_distinct_emails_8w']\n",
    "binary_cols = ['fraud_bool', 'email_is_free', 'phone_home_valid', 'phone_mobile_valid', 'has_other_cards', 'foreign_request', 'keep_alive_session']\n",
    "discrete_cols = ['payment_type', 'employment_status', 'housing_status', 'source', 'device_os', 'month']\n",
    "normal_distribution_cols = ['zip_count_4w', 'velocity_6h', 'velocity_24h', 'date_of_birth_distinct_emails_4w', 'credit_risk_score']\n",
    "target_col = 'fraud_bool'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing Outliers\n",
    "Outliers can take many different forms in a dataset. In some cases, outliers may be extreme values that fall far outside the expected range of the data, while in other cases, outliers may appear as discontinuities or gaps in the data.\n",
    "\n",
    "In this particular dataset, it has been observed that there are some columns - customer_age, days_since_request, intended_balcon_amount, and proposed_credit_limit - that contain discontinuous points, based on the boxplots. These points may represent missing data or errors in data collection, or they may be indicative of some other pattern in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_analyze = ['customer_age', 'days_since_request', 'intended_balcon_amount', 'proposed_credit_limit', 'device_distinct_emails_8w']\n",
    "\n",
    "# create a grid of subplots using seaborn\n",
    "n_cols = 2  # number of columns in the grid\n",
    "n_rows = (len(columns_to_analyze) + n_cols - 1) // n_cols  # number of rows in the grid\n",
    "fig, axes = plt.subplots(nrows=n_rows, ncols=n_cols, figsize=(25, 5*n_rows))\n",
    "\n",
    "# loop through the columns and create a boxplot for each one\n",
    "for i, col in enumerate(columns_to_analyze):\n",
    "    row_idx = i // n_cols  # row index for this subplot\n",
    "    col_idx = i % n_cols  # column index for this subplot\n",
    "    ax = sns.scatterplot(x=col, y=target_col, data=df, ax=axes[row_idx, col_idx])\n",
    "    ax.set_title(col)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon examining the selected columns, we did not find significant differences in the values between the fraud and non-fraud categories. This suggests that outliers in these variables are not particularly informative or indicative of fraud. Instead, they may be a result of random variations or noise in the data.\n",
    "\n",
    "Considering these findings, it may not be necessary to remove outliers from these variables to improve the accuracy of your analysis or model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Matrix\n",
    "We can calculate the correlation matrix between the numerical features in the dataset to see how they are related to each other. This can give us some insight into which features are most important for predicting fraud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the correlation matrix\n",
    "corr_matrix = df[continuous_cols].corr()\n",
    "\n",
    "# Create a heatmap of the correlation matrix\n",
    "fig, ax = plt.subplots(figsize=(25, 30))\n",
    "ax = sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f', annot_kws={'size': 10})\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Add more details about the correlation matrix"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation between continuous variables and our target (binary) variable"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the distributions between fraud and non-fraud categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of columns and rows in the grid\n",
    "num_cols = 3\n",
    "num_rows = (len(continuous_cols) + num_cols - 1) // num_cols\n",
    "\n",
    "# Create a grid of subplots\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, 10))\n",
    "\n",
    "# Flatten the axes array\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Create box plots for each continuous variable\n",
    "for i, col in enumerate(continuous_cols):\n",
    "    ax = axes[i]\n",
    "    sns.boxplot(x=target_col, y=col, data=df, ax=ax)\n",
    "    ax.set_title(f\"{col} by {target_col}\")\n",
    "\n",
    "# Remove any empty subplots\n",
    "if len(continuous_cols) < len(axes):\n",
    "    for j in range(len(continuous_cols), len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "\n",
    "# Adjust the layout\n",
    "fig.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see there is slighty different distributions on the income, date_of_birth_distinct_emails_4w, proposed_credit_limit columns between fraud and non-fraud categories. We can also see that the distributions of the other variables are similar between the two categories. This suggests that these variables may not be particularly informative for predicting fraud, however the those three variables may be useful in predicting fraud."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### T-test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The t-test is commonly used to compare the means of two groups or conditions. It is suitable for situations where we have a continuous outcome variable and a binary variable, our target variable.\n",
    "\n",
    "The t-test assumes the continuous outcome variable is normally distributed and that the variance of the two groups is equal. Therefore, we perform Levene's test to check the equality of variances between the groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Levene\\'s Test')\n",
    "\n",
    "# Split the data into two groups based on the binary variable\n",
    "for col in normal_distribution_cols:\n",
    "    group_1 = df[df[target_col] == 0][col]\n",
    "    group_2 = df[df[target_col] == 1][col]\n",
    "\n",
    "    statistic, p_value = stats.levene(group_1, group_2)\n",
    "\n",
    "    # Print the test results\n",
    "    print(f\"Test Statistic: {statistic:.4f}\")\n",
    "    print(f\"P-value: {p_value:.4f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the p-value from Levene's test is below the chosen significance level (e.g., 0.05), it indicates a statistically significant difference in variances between the groups. This would violate the assumption of equal variances required for the t-test, and thus we would need to explore alternative tests.\n",
    "\n",
    "Looking at the Levene's test results, we analize that the variance between the two groups is statistically significant."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the Mann-Whitney U test, which does not assume equal variances."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mann-Whitney U Test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Mann-Whitney U test is a non-parametric test that does not rely on the assumption of equal variances. It compares the distributions of two independent groups based on the ranks of the observations.\n",
    "\n",
    "This test is appropriate when the data do not meet the assumptions required for the T-test, such as when the data are non-normally distributed or the variances are unequal."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "effect_sizes = []\n",
    "results_mannwhitneyu = []\n",
    "\n",
    "for col in continuous_cols:\n",
    "    group_1 = df[df[target_col] == 0][col]\n",
    "    group_2 = df[df[target_col] == 1][col]\n",
    "\n",
    "    stat, p_value_mannwhitneyu = mannwhitneyu(group_1, group_2)\n",
    "    results_mannwhitneyu.append((col, stat, p_value_mannwhitneyu))\n",
    "\n",
    "    # Calculate Cohen's U3\n",
    "    u3 = stat / (len(group_1) * len(group_2))\n",
    "    effect_sizes.append((col, u3, p_value_mannwhitneyu))\n",
    "\n",
    "df_mannwhitneyu = pd.DataFrame(results_mannwhitneyu, columns=['Variable', 'Mann-Whitney U', 'P-value (Mann-Whitney U)'])\n",
    "effect_sizes_df = pd.DataFrame(effect_sizes, columns=['Variable', \"Cohen's U3\", 'P-value (Mann-Whitney U)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mannwhitneyu.plot(x='Variable', y='P-value (Mann-Whitney U)', kind='bar')\n",
    "plt.title('Mann-Whitney U')\n",
    "plt.ylabel('P-value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_mannwhitneyu)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting p-value from the test represents the probability of observing a U statistic as extreme as the one calculated, assuming that the null hypothesis is true (i.e., the distributions of the two groups are identical). A small p-value indicates strong evidence against the null hypothesis and suggests that there is a significant difference between the two groups (fraud and non-fraud)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now visualize the magnitude of the differences between the fraud and non-fraud categories for each variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_names = effect_sizes_df['Variable']\n",
    "cohens_u3 = effect_sizes_df[\"Cohen's U3\"]\n",
    "p_values = effect_sizes_df['P-value (Mann-Whitney U)']\n",
    "\n",
    "bar_width = 0.35\n",
    "\n",
    "x_indices = np.arange(len(variable_names))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "bar1 = ax.bar(x_indices - bar_width, p_values, bar_width, label='P-value')\n",
    "bar2 = ax.bar(x_indices + bar_width, cohens_u3, bar_width, label=\"Cohen's U3\")\n",
    "\n",
    "ax.set_xticks(x_indices + bar_width)\n",
    "ax.set_xticklabels(variable_names, rotation=90)\n",
    "\n",
    "ax.set_ylabel('Value')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(effect_sizes_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Cohen's U3 effect size of 0.6 is considered relatively large, it indicates a substatial difference between the two groups, value we can see on the date_of_birth_distinct_emails_4w. The p-values are so close to 0.0 that we they do not appear visible in the plot, however this means the confidence on the difference between the two groups is very high."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Point-Biserial Correlation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_correlation = []\n",
    "\n",
    "for col in continuous_cols:\n",
    "    correlation, p_value_correlation = stats.pointbiserialr(df[col], df[target_col])\n",
    "    results_correlation.append((col, correlation, p_value_correlation))\n",
    "\n",
    "df_correlation = pd.DataFrame(results_correlation, columns=['Variable', 'Correlation', 'P-value (Correlation)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_correlation.plot(x='Variable', y='Correlation', kind='bar')\n",
    "plt.title('Point-Biserial Correlation: Correlations')\n",
    "plt.ylabel('Correlation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(10, 6))\n",
    "\n",
    "# Boxplot 1\n",
    "sns.boxplot(x=target_col, y='days_since_request', data=df[df[target_col] == 0][[target_col, 'days_since_request']], ax=axes[0])\n",
    "axes[0].set_title('Boxplot - Continuous Variable 1')\n",
    "axes[0].set_xlabel('Binary Variable')\n",
    "axes[0].set_ylabel('Continuous Variable 1')\n",
    "\n",
    "# Boxplot 2\n",
    "sns.boxplot(x=target_col, y='days_since_request', data=df[df[target_col] == 1][[target_col, 'days_since_request']], ax=axes[1])\n",
    "axes[1].set_title('Boxplot - Continuous Variable 2')\n",
    "axes[1].set_xlabel('Binary Variable')\n",
    "axes[1].set_ylabel('Continuous Variable 2')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation between categorical variables and our target (binary) variable"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chi-square test\n",
    "We want to perceive the relation between the categorical features in the dataset to see how they are related to the target variable. This can give us some insight into which features are most important for predicting fraud.\n",
    "\n",
    "(Note: chosen significance level (alpha) = 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = [\"Variable\", \"Chi-square Statistic\", \"Degrees of freedom\", \"P-value\"]\n",
    "\n",
    "data = []\n",
    "\n",
    "for col in discrete_cols:\n",
    "    contingency_table = pd.crosstab(df[target_col], df[col])\n",
    "\n",
    "    chi2_stat, p_val, dof, expected = stats.chi2_contingency(contingency_table)\n",
    "\n",
    "    data.append([col, chi2_stat, dof, p_val])\n",
    "\n",
    "print(tabulate(data, headers=col_names, tablefmt=\"fancy_grid\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this table, we have 4 columns:\n",
    "- **Variable** <br>\n",
    "Name of the variable being analised.\n",
    "\n",
    "- **Chi-square Statistic** <br>\n",
    "Measures the overall discrepancy between the observed frequencies and the expected frequencies under the assumption of independence. A larger chi-square statistic suggests a greater difference between the observed and expected frequencies.\n",
    "\n",
    "- **Degree of freedom** <br>\n",
    "Represent the number of categories in the variables minus 1. In the context of a chi-square test, it determines the critical values or the distribution of the chi-square statistic. The degrees of freedom help in assessing the statistical significance of the chi-square test.\n",
    "\n",
    "- **P-value** <br>\n",
    "Indicates the statistical significance of the association, if the p-value is below a chosen significance level (alpha < 0.05), it suggests a significant association."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through analysis of the results, we can conclude that, for all the variables analysed, in none of them the p-value is higher or equal to the chosen significance level (alpha = 0.05), only lower. What we can perceive from the fact that the p-value is zero is that the observed results are highly unlikely to be due to random chance, and there is a significant relationship or effect present in the data. As a result, we can only take conclusions based on the chi-square Statistic and the degrees of freedom of each variable. Normally, the higher the degree of freedom the higher the chi-square statistic tends to be. By observing the table, we can understand that some variables have stronger relation with the target variable, such as \"device_os\" and \"housing_status\"."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering and Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of dataframes we will disponibilize throughout the notebook\n",
    "\n",
    "df_oversampled = -1\n",
    "df_undersampled = -1\n",
    "df_all_features = -1\n",
    "df_without_most_correlated_features = -1\n",
    "df_without_least_correlated_features = -1\n",
    "\n",
    "dataframes = [df_oversampled, df_undersampled, df_all_features, df_without_most_correlated_features, df_without_least_correlated_features]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Encoding\n",
    "In order to encode some of our variables, we looked up some of the most used encoding methods. We ended up choosing Ordinal Encoding for High-Cardinality Variables because we have categorical variables with a high number of unique values (high-cardinality). Being that using One-Hot Encoding might lead to a large number of resulting columns and given that we want to reduce the number of columns, One-Hot Encoding turns out not being the best choice. So we decided to use Ordinal Encoding, which assigns unique integers to each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = df.copy()\n",
    "\n",
    "columns_to_encode = ['payment_type', 'employment_status', 'housing_status', 'source', 'device_os']\n",
    "\n",
    "encoder = OrdinalEncoder()\n",
    "\n",
    "dataset[columns_to_encode] = encoder.fit_transform(dataset[columns_to_encode])\n",
    "\n",
    "#print(dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting relevant features\n",
    "In this step we select the features that have the most impact on our target variable (\"fraud_bool\"). "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO - Add more details about the feature selection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imbalanced Dataset\n",
    "Dealing with imbalanced datasets is an important aspect of machine learning. In this section, we will explore some techniques for dealing with imbalanced datasets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(report, cm, title):\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.text(1, 3.2, str(report), fontsize=12, ha='center')\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Reds')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.ylabel('True label')\n",
    "    plt.suptitle(title)\n",
    "    plt.show()\n",
    "\n",
    "training_data = dataset.sample(frac=0.1, random_state=1)\n",
    "\n",
    "# Separate the features and the target variable\n",
    "X = training_data.drop('fraud_bool', axis=1)\n",
    "y = training_data['fraud_bool']\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=1, stratify=y)\n",
    "\n",
    "# Shuffle the training data\n",
    "\n",
    "X_train, y_train = shuffle(X_train, y_train, random_state=1)\n",
    "\n",
    "# Train a Random Forest classifier on the original data\n",
    "clf_original = RandomForestClassifier(random_state=1)\n",
    "clf_original.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Evaluate the original model on the test set\n",
    "y_pred_original = clf_original.predict(X_test)\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred_original)\n",
    "\n",
    "report = classification_report(y_test, y_pred_original, zero_division=0)\n",
    "# Plot the confusion matrix\n",
    "plot_confusion_matrix(report, cm, 'Confusion Matrix - Original Data')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original dataset without any techniques applied to deal with the imbalanced dataset results in a model that is biased towards predicting non-fraud cases. As we can see from the confusion matrix, the model is not able to predict any fraud cases correctly. The f1-score is 0.00, indicating poor overall performance in identifying fraudulent cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Apply Random Under-sampling to balance the data\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "X_resampled_rus, y_resampled_rus = rus.fit_resample(X_train, y_train)\n",
    "\n",
    "\n",
    "# Train a Random Forest classifier on the balanced data using Random Under-sampling\n",
    "clf_rus = RandomForestClassifier(random_state=42)\n",
    "clf_rus.fit(X_resampled_rus, y_resampled_rus)\n",
    "\n",
    "# Evaluate the Random Under-sampling model on the test set\n",
    "y_pred_rus = clf_rus.predict(X_test)\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred_rus)\n",
    "\n",
    "report = classification_report(y_test, y_pred_rus, zero_division=0)\n",
    "# Plot the confusion matrix\n",
    "plot_confusion_matrix(report, cm, 'Confusion Matrix - Random Under-sampling')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplying a undersampling technique, we can see that the model is now able to predict some fraud cases correctly. The recall for the minority class improves significantly (0.75) indicating that the model captures a higher proportion of fraudulent cases and the f1-score improves to 0.08 indicating better overall performance in identifying fraudulent cases, but still not good enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Random Over-sampling to balance the data\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "X_resampled_ros, y_resampled_ros = ros.fit_resample(X_train, y_train)\n",
    "\n",
    "# Train a Random Forest classifier on the balanced data using Random Over-sampling\n",
    "clf_ros = RandomForestClassifier(random_state=42)\n",
    "clf_ros.fit(X_resampled_ros, y_resampled_ros)\n",
    "\n",
    "# Evaluate the Random Over-sampling model on the test set\n",
    "y_pred_ros = clf_ros.predict(X_test)\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred_ros)\n",
    "\n",
    "report = classification_report(y_test, y_pred_ros, zero_division=0)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plot_confusion_matrix(report, cm, 'Confusion Matrix - Random Over-sampling')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplying a oversampling technique, we can see that the model doesn't improve much. The recall for the minority class is 0.01 and the f1-score is 0.03, which doesn't improve a lot from the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply SMOTE to balance the data\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled_smote, y_resampled_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Train a Random Forest classifier on the balanced data using SMOTEENN\n",
    "clf_smote = RandomForestClassifier(random_state=42)\n",
    "clf_smote.fit(X_resampled_smote, y_resampled_smote)\n",
    "\n",
    "# Evaluate the SMOTE model on the test set\n",
    "y_pred_smote = clf_smote.predict(X_test)\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred_smote)\n",
    "\n",
    "report = classification_report(y_test, y_pred_smote, zero_division=0)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plot_confusion_matrix(report, cm, 'Confusion Matrix - SMOTE')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Concerning the Smote technique, although the improvements were relatively small we noticed a slight improvement regarding the original dataset.\n",
    "The precision, recall, and F1-score for the minority class are still quite low compared to the majority class."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Takeaways from analysis\n",
    "\n",
    "TODO: Add more details about the main takeaways from analysis and preprocessing phase"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, the results confirm that the original dataset suffers from severe class imbalance, leading to poor performance in identifying fraudulent cases. While the applied techniques (under-sampling, over-sampling, and SMOTE) show some improvements in identifying fraudulent cases compared to the original dataset, the performance remains limited. Further analysis and experimentation may be required to develop a more effective model for fraud detection in this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the false positive rate (FPR), true positive rate (TPR), and threshold\n",
    "fpr_undersampling, tpr_undersampling, thresholds = roc_curve(y_test, y_pred_rus)\n",
    "fpr_smote, tpr_smote, thresholds = roc_curve(y_test, y_pred_smote)\n",
    "fpr_oversampling, tpr_oversampling, thresholds = roc_curve(y_test, y_pred_ros)\n",
    "\n",
    "auc_undersampling = auc(fpr_undersampling, tpr_undersampling)\n",
    "auc_smote = auc(fpr_smote, tpr_smote)\n",
    "auc_oversampling = auc(fpr_oversampling, tpr_oversampling)\n",
    "\n",
    "# Plot the ROC curve\n",
    "plt.figure(figsize=(5,5), dpi=100)\n",
    "\n",
    "plt.plot(fpr_undersampling, tpr_undersampling, color='red', lw=2, label='ROC curve - Undersampling (area = %0.2f)' % auc_undersampling)\n",
    "plt.plot(fpr_smote, tpr_smote, color='green', lw=2, label='ROC curve - SMOTE (area = %0.2f)' % auc_smote)\n",
    "plt.plot(fpr_oversampling, tpr_oversampling, color='blue', lw=2, label='ROC curve - Oversampling (area = %0.2f)' % auc_oversampling)\n",
    "plt\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Modeling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing modeling techniques\n",
    "The modeling techniques chosen are: \n",
    "- Naive Bayes\n",
    "- Decision Tree\n",
    "- k-Nearest Neighbours (KNN)\n",
    "- Logistic Regression\n",
    "- Support Vectors (SVM)\n",
    "- Random Forest"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the dataset into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset.drop('fraud_bool', axis=1) #Features \n",
    "y = dataset['fraud_bool'] # Labels\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying models and evaluating performance\n",
    "In this section, we apply the models chosen and evaluate their performance with appropriate metrics.\n",
    "\n",
    "The metrics chosen are the following:\n",
    "- Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes\n",
    "Naive Bayes is a classification algorithm based on Bayes' theorem. It assumes that features are independent and calculates the probability of an instance belonging to a class. It's computationally efficient, works well with high-dimensional data, and performs best when the independence assumption holds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate and train the Naive Bayes classifier\n",
    "model = GaussianNB()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "\n",
    "precision = precision_score(y_test, y_pred, zero_division=0)\n",
    "\n",
    "recall = recall_score(y_test, y_pred, zero_division=0)\n",
    "\n",
    "print('Accuracy:', accuracy)\n",
    "print('F1 Score:', f1)\n",
    "print('Precision:', precision)\n",
    "print('Recall:', recall)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree\n",
    "Decision Trees are classification algorithms that create a tree-like model of decisions. Each internal node represents a feature, and each leaf node represents a class label. They split the data based on feature values to create homogeneous subsets. When making predictions, a new instance traverses the tree to a leaf node, and the corresponding class label is assigned. Decision Trees are interpretable and handle categorical and numerical data, capturing complex decision boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### k-Nearest Neighbours (KNN) \n",
    "k-Nearest Neighbors (KNN) is a classification algorithm that predicts the class of an instance based on its k nearest neighbors in the feature space. It assumes that instances with similar features tend to belong to the same class. During training, KNN stores the feature vectors and their corresponding class labels. When making predictions, it finds the k nearest neighbors to the target instance and assigns the majority class among those neighbors as the predicted class. KNN is a simple and versatile algorithm that can handle both linear and non-linear classification tasks, making it useful in various applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KNeighborsClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression\n",
    "Logistic Regression is a classification algorithm that predicts the probability of an instance belonging to a specific class. It uses a sigmoid function to map input features to a binary output. By fitting a decision boundary during training, Logistic Regression separates the classes. When making predictions, it calculates the probability of an instance belonging to the positive class and applies a threshold for classification. Logistic Regression is a simple and effective algorithm suitable for binary classification tasks, handling linearly and non-linearly separable data with appropriate transformations or kernel functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vectors (SVM)\n",
    "Support Vector Machines (SVM) are powerful classifiers that can handle both linear and non-linear classification tasks. They work by finding an optimal hyperplane that maximally separates the data points of different classes. SVMs also offer various kernels (e.g., linear, polynomial, radial basis function) to capture complex relationships between the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVC()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest \n",
    "Random Forest is an ensemble learning method that combines multiple decision trees to make predictions. Each decision tree is trained on a random subset of the data, and the final prediction is determined by aggregating the predictions of individual trees. Random Forests are effective in handling complex datasets, capturing non-linear relationships, and mitigating overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models Evaluation and Validation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models performance on testing data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying cross-validation techniques"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation and Insights"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
