{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Intelligence for Business\n",
    "\n",
    "This Jupyter notebook performs Exploratory Data Analysis (EDA) on one of the six synthetic tabular datasets in the Bank Account Fraud (BAF) suite of datasets. The BAF datasets were published at NeurIPS 2022 and are intended to provide a realistic, complete, and robust test bed to evaluate novel and existing methods in machine learning (ML) and fair ML."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Description"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of this notebook is to provide an overview over the dataset and prepare it to be used for training and evaluating ML models."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The BAF suite of datasets comprises a total of 6 different synthetic bank account fraud tabular datasets. The datasets are realistic, based on a present-day real-world dataset for fraud detection, and each dataset has distinct controlled types of bias. Additionally, the datasets have an imbalanced setting with an extremely low prevalence of positive class, contain temporal data and observed distribution shifts, and have privacy-preserving features to protect the identity of potential applicants.\n",
    "\n",
    "In this notebook, we will be exploring one of the datasets in the BAF suite, the Base.csv dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "from tabulate import tabulate\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import OrdinalEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV, KFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc, accuracy_score, precision_score, recall_score, make_scorer, f1_score, roc_auc_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression, Lasso\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Understanding and Exploration"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and describing the dataset\n",
    "We start by loading the dataset into a Pandas DataFrame and displaying its first few rows using the head() function. We then display some basic statistics of the dataset using the describe() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Read in the data\n",
    "    df = pd.read_csv('../dataset/Base.csv', header=0)\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: File not found.\")\n",
    "except pd.errors.EmptyDataError:\n",
    "    print(\"Error: Empty DataFrame.\")\n",
    "except pd.errors.ParserError:\n",
    "    print(\"Error: Parsing error occurred.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {str(e)}\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boxplot\n",
    "We then use the Seaborn library to create boxplots of the numerical columns in the dataset. Boxplots are used to visualize the distribution and outliers of each numerical column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the list of numerical columns\n",
    "num_cols = df.select_dtypes(include=['float', 'int']).columns.tolist()\n",
    "\n",
    "# create a grid of subplots using seaborn\n",
    "n_cols = 3  # number of columns in the grid\n",
    "n_rows = (len(num_cols) + n_cols - 1) // n_cols  # number of rows in the grid\n",
    "fig, axes = plt.subplots(nrows=n_rows, ncols=n_cols, figsize=(25, 5*n_rows))\n",
    "\n",
    "# loop through the columns and create a boxplot for each one\n",
    "for i, col in enumerate(num_cols):\n",
    "    row_idx = i // n_cols  # row index for this subplot\n",
    "    col_idx = i % n_cols  # column index for this subplot\n",
    "    ax = sns.boxplot(data=df[col], ax=axes[row_idx, col_idx])\n",
    "    ax.set_title(col)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the analysis of the boxplots, we have identified a column that contains only a single value, with an equal value for all data inputs. Since this column does not provide any meaningful variation or information, it is recommended to remove it from the dataset before proceeding with further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_to_remove = 'device_fraud_count'\n",
    "\n",
    "# drop the column from the dataframe\n",
    "df = df.drop(column_to_remove, axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histograms\n",
    "We can also create histograms of the numerical columns to see the distribution of each feature. This can help us identify any features that may need to be transformed to achieve a normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.hist(bins=20, figsize=(25, 20))\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see some columns follow a normal distribution. Those columns are zip_count_4w, velocity_6h, velocity_24h, date_of_birth_distinct_emails_4w, credit_risk_score."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Plot\n",
    "Finally, we can create a count plot to visualize the distribution of the target variable (fraud). This can help us identify the class imbalance in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a count plot of the target variable\n",
    "sns.countplot(x='fraud_bool', data=df)\n",
    "plt.title('Target Variable Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Division of Variables\n",
    "The variables have been grouped as follows:\n",
    "- Target variable: Variable of interest in the project\n",
    "- Continuous variables: These variables represent quantitative measurements\n",
    "- Categorical variables: These variables represent a finite set of possible values\n",
    "- Binary variables: These variables have two distinct values, normally representing a yes/no condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous_cols = ['income', 'name_email_similarity', 'prev_address_months_count', 'current_address_months_count', 'customer_age', 'days_since_request', 'intended_balcon_amount', 'zip_count_4w', 'velocity_6h', 'velocity_24h', 'velocity_4w', 'bank_branch_count_8w', 'date_of_birth_distinct_emails_4w', 'credit_risk_score', 'bank_months_count', 'proposed_credit_limit', 'session_length_in_minutes', 'device_distinct_emails_8w']\n",
    "binary_cols = ['fraud_bool', 'email_is_free', 'phone_home_valid', 'phone_mobile_valid', 'has_other_cards', 'foreign_request', 'keep_alive_session']\n",
    "discrete_cols = ['payment_type', 'employment_status', 'housing_status', 'source', 'device_os', 'month']\n",
    "normal_distribution_cols = ['zip_count_4w', 'velocity_6h', 'velocity_24h', 'date_of_birth_distinct_emails_4w', 'credit_risk_score']\n",
    "target_col = 'fraud_bool'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing Outliers\n",
    "Outliers can take many different forms in a dataset. In some cases, outliers may be extreme values that fall far outside the expected range of the data, while in other cases, outliers may appear as discontinuities or gaps in the data.\n",
    "\n",
    "In this particular dataset, it has been observed that there are some columns - customer_age, days_since_request, intended_balcon_amount, and proposed_credit_limit - that contain discontinuous points, based on the boxplots. These points may represent missing data or errors in data collection, or they may be indicative of some other pattern in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_analyze = ['customer_age', 'days_since_request', 'intended_balcon_amount', 'proposed_credit_limit', 'device_distinct_emails_8w']\n",
    "\n",
    "# create a grid of subplots using seaborn\n",
    "n_cols = 2  # number of columns in the grid\n",
    "n_rows = (len(columns_to_analyze) + n_cols - 1) // n_cols  # number of rows in the grid\n",
    "fig, axes = plt.subplots(nrows=n_rows, ncols=n_cols, figsize=(25, 5*n_rows))\n",
    "\n",
    "# loop through the columns and create a boxplot for each one\n",
    "for i, col in enumerate(columns_to_analyze):\n",
    "    row_idx = i // n_cols  # row index for this subplot\n",
    "    col_idx = i % n_cols  # column index for this subplot\n",
    "    ax = sns.scatterplot(x=col, y=target_col, data=df, ax=axes[row_idx, col_idx])\n",
    "    ax.set_title(col)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon examining the selected columns, we did not find significant differences in the values between the fraud and non-fraud categories. This suggests that outliers in these variables are not particularly informative or indicative of fraud. Instead, they may be a result of random variations or noise in the data.\n",
    "\n",
    "Considering these findings, it may not be necessary to remove outliers from these variables to improve the accuracy of your analysis or model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Matrix\n",
    "We can calculate the correlation matrix between the numerical features in the dataset to see how they are related to each other. This can give us some insight into which features are most important for predicting fraud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the correlation matrix\n",
    "corr_matrix = df[continuous_cols].corr()\n",
    "\n",
    "# Create a heatmap of the correlation matrix\n",
    "fig, ax = plt.subplots(figsize=(25, 30))\n",
    "ax = sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f', annot_kws={'size': 10})\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Takeaways:\n",
    "- income has a positive correlation with customer_age and credit_risk_score, indicating that higher income tends to be associated with older customers and higher credit risk scores.\n",
    "- prev_address_months_count and current_address_months_count have a negative correlation, indication that customers with longer previous addresses tend to have shorter current addresses.\n",
    "- customer_age has a negative correlation with date_of_birth_distinct_emails_4w, indicating that a higher number of distinct emails for applicants with the same date of birth in the last 4 weeks may be associated with younger customers.\n",
    "- the variables related to velocity (velocity_6h, velocity_24h, velocity_4w) have positive correlations with each other, indicating that higher velocity in one timeframe is generally associated with higher velocity in other timeframes.\n",
    "- credit_risk_score has a strong positive correlation with proposed_credit_limit, indicating that higher credit risk scores are more likely to have higher proposed credit limits."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation between continuous variables and our target (binary) variable"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing the distributions between fraud and non-fraud categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of columns and rows in the grid\n",
    "num_cols = 3\n",
    "num_rows = (len(continuous_cols) + num_cols - 1) // num_cols\n",
    "\n",
    "# Create a grid of subplots\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, 10))\n",
    "\n",
    "# Flatten the axes array\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Create box plots for each continuous variable\n",
    "for i, col in enumerate(continuous_cols):\n",
    "    ax = axes[i]\n",
    "    sns.boxplot(x=target_col, y=col, data=df, ax=ax)\n",
    "    ax.set_title(f\"{col} by {target_col}\")\n",
    "\n",
    "# Remove any empty subplots\n",
    "if len(continuous_cols) < len(axes):\n",
    "    for j in range(len(continuous_cols), len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "\n",
    "# Adjust the layout\n",
    "fig.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see there is slighty different distributions on the income, date_of_birth_distinct_emails_4w, proposed_credit_limit columns between fraud and non-fraud categories. We can also see that the distributions of the other variables are similar between the two categories. This suggests that these variables may not be particularly informative for predicting fraud, however the those three variables may be useful in predicting fraud."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### T-test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The t-test is commonly used to compare the means of two groups or conditions. It is suitable for situations where we have a continuous outcome variable and a binary variable, our target variable.\n",
    "\n",
    "The t-test assumes the continuous outcome variable is normally distributed and that the variance of the two groups is equal. Therefore, we perform Levene's test to check the equality of variances between the groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Levene\\'s Test')\n",
    "\n",
    "# Split the data into two groups based on the binary variable\n",
    "for col in normal_distribution_cols:\n",
    "    group_1 = df[df[target_col] == 0][col]\n",
    "    group_2 = df[df[target_col] == 1][col]\n",
    "\n",
    "    statistic, p_value = stats.levene(group_1, group_2)\n",
    "\n",
    "    # Print the test results\n",
    "    print(f\"Test Statistic: {statistic:.4f}\")\n",
    "    print(f\"P-value: {p_value:.4f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the p-value from Levene's test is below the chosen significance level (e.g., 0.05), it indicates a statistically significant difference in variances between the groups. This would violate the assumption of equal variances required for the t-test, and thus we would need to explore alternative tests.\n",
    "\n",
    "Looking at the Levene's test results, we analize that the variance between the two groups is statistically significant."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the Mann-Whitney U test, which does not assume equal variances."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mann-Whitney U Test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Mann-Whitney U test is a non-parametric test that does not rely on the assumption of equal variances. It compares the distributions of two independent groups based on the ranks of the observations.\n",
    "\n",
    "This test is appropriate when the data do not meet the assumptions required for the T-test, such as when the data are non-normally distributed or the variances are unequal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "effect_sizes = []\n",
    "results_mannwhitneyu = []\n",
    "\n",
    "for col in continuous_cols:\n",
    "    group_1 = df[df[target_col] == 0][col]\n",
    "    group_2 = df[df[target_col] == 1][col]\n",
    "\n",
    "    stat, p_value_mannwhitneyu = stats.mannwhitneyu(group_1, group_2)\n",
    "    results_mannwhitneyu.append((col, stat, p_value_mannwhitneyu))\n",
    "\n",
    "    # Calculate Cohen's U3\n",
    "    u3 = stat / (len(group_1) * len(group_2))\n",
    "    effect_sizes.append((col, u3, p_value_mannwhitneyu))\n",
    "\n",
    "df_mannwhitneyu = pd.DataFrame(results_mannwhitneyu, columns=['Variable', 'Mann-Whitney U', 'P-value (Mann-Whitney U)'])\n",
    "effect_sizes_df = pd.DataFrame(effect_sizes, columns=['Variable', \"Cohen's U3\", 'P-value (Mann-Whitney U)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mannwhitneyu.plot(x='Variable', y='P-value (Mann-Whitney U)', kind='bar')\n",
    "plt.title('Mann-Whitney U')\n",
    "plt.ylabel('P-value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_mannwhitneyu)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting p-value from the test represents the probability of observing a U statistic as extreme as the one calculated, assuming that the null hypothesis is true (i.e., the distributions of the two groups are identical). A small p-value indicates strong evidence against the null hypothesis and suggests that there is a significant difference between the two groups (fraud and non-fraud)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now visualize the magnitude of the differences between the fraud and non-fraud categories for each variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_names = effect_sizes_df['Variable']\n",
    "cohens_u3 = effect_sizes_df[\"Cohen's U3\"]\n",
    "p_values = effect_sizes_df['P-value (Mann-Whitney U)']\n",
    "\n",
    "bar_width = 0.35\n",
    "\n",
    "x_indices = np.arange(len(variable_names))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "bar1 = ax.bar(x_indices - bar_width, p_values, bar_width, label='P-value')\n",
    "bar2 = ax.bar(x_indices + bar_width, cohens_u3, bar_width, label=\"Cohen's U3\")\n",
    "\n",
    "ax.set_xticks(x_indices + bar_width)\n",
    "ax.set_xticklabels(variable_names, rotation=90)\n",
    "\n",
    "ax.set_ylabel('Value')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(effect_sizes_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Cohen's U3 effect size of 0.6 is considered relatively large, it indicates a substatial difference between the two groups, value we can see on the date_of_birth_distinct_emails_4w. The p-values are so close to 0.0 that we they do not appear visible in the plot, however this means the confidence on the difference between the two groups is very high."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Point-Biserial Correlation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we perform a point-biserial correlation analysis to examine the relationship between the target variable and the continuous variables in the dataset.\n",
    "\n",
    "First, we calculate the point-biserial correlation coefficients and p-values for each continuous variable. The correlation coefficient measures the strength and direction of the relationship, while the p-value indicates the statistical significance of the correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming your DataFrame is named 'df' and the target variable column is 'target_col'\n",
    "binary_var = df[target_col]\n",
    "\n",
    "coefficients = []\n",
    "p_values = []\n",
    "\n",
    "for var in continuous_cols:\n",
    "    correlation, p_value = stats.pointbiserialr(binary_var, df[var])\n",
    "    coefficients.append(correlation)\n",
    "    p_values.append(p_value)\n",
    "\n",
    "# Create subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 6))\n",
    "\n",
    "# Plot coefficients\n",
    "ax1.bar(np.arange(len(continuous_cols)), coefficients, tick_label=continuous_cols)\n",
    "ax1.set_xlabel('Continuous Variables')\n",
    "ax1.set_ylabel('Point-Biserial Correlation Coefficient')\n",
    "ax1.set_title('Point-Biserial Correlation Coefficients')\n",
    "ax1.tick_params(axis='x', rotation=60)  # Rotate x-axis labels by 60 degrees\n",
    "\n",
    "# Plot p-values\n",
    "ax2.bar(np.arange(len(continuous_cols)), p_values, tick_label=continuous_cols)\n",
    "ax2.set_xlabel('Continuous Variables')\n",
    "ax2.set_ylabel('p-value')\n",
    "ax2.set_title('P-values')\n",
    "ax2.tick_params(axis='x', rotation=60)  # Rotate x-axis labels by 60 degrees\n",
    "\n",
    "# Adjust spacing between subplots\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bar plots display the correlation coefficients and p-values side by side. The x-axis represents the continuous variables, while the y-axis represents the correlation coefficient or p-value. The coefficients indicate the degree of association between the continuous variables and the target variable, while the p-values help assess the significance of these associations.\n",
    "\n",
    "From the analysis, we can gain insights into which continuous variables have a strong or weak relationship with the target variable. Higher coefficients indicate a stronger association, while lower p-values suggest a more significant relationship."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the point-biserial correlation test, the variable days_since_request has a p-value above 0.5, which means there is no evidence to support a significant relationship between this variable and the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(10, 6))\n",
    "\n",
    "# Boxplot 1\n",
    "sns.boxplot(x=target_col, y='days_since_request', data=df[df[target_col] == 0][[target_col, 'days_since_request']], ax=axes[0])\n",
    "axes[0].set_title('Boxplot - Days since request, NOT Fraud')\n",
    "axes[0].set_xlabel(target_col)\n",
    "axes[0].set_ylabel('days_since_request')\n",
    "\n",
    "# Boxplot 2\n",
    "sns.boxplot(x=target_col, y='days_since_request', data=df[df[target_col] == 1][[target_col, 'days_since_request']], ax=axes[1])\n",
    "axes[1].set_title('Boxplot - Days since request, Fraud')\n",
    "axes[1].set_xlabel(target_col)\n",
    "axes[1].set_ylabel('days_since_request')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of the 'days_since_request' variable, the point-biserial correlation analysis and boxplot visualization did not reveal any clear difference between fraud and non-fraud cases. The p-value for this variable was above 0.5, indicating no significant association with the target variable."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation between categorical variables and our target (binary) variable"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chi-square test\n",
    "We want to perceive the relation between the categorical features in the dataset to see how they are related to the target variable. This can give us some insight into which features are most important for predicting fraud.\n",
    "\n",
    "(Note: chosen significance level (alpha) = 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = [\"Variable\", \"Chi-square Statistic\", \"Degrees of freedom\", \"P-value\"]\n",
    "\n",
    "data = []\n",
    "\n",
    "for col in discrete_cols + binary_cols:\n",
    "    if(col == target_col): continue\n",
    "\n",
    "    contingency_table = pd.crosstab(df[target_col], df[col])\n",
    "\n",
    "    chi2_stat, p_val, dof, expected = stats.chi2_contingency(contingency_table)\n",
    "\n",
    "    data.append([col, chi2_stat, dof, p_val])\n",
    "\n",
    "print(tabulate(data, headers=col_names, tablefmt=\"fancy_grid\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this table, we have 4 columns:\n",
    "- **Variable** <br>\n",
    "Name of the variable being analised.\n",
    "\n",
    "- **Chi-square Statistic** <br>\n",
    "Measures the overall discrepancy between the observed frequencies and the expected frequencies under the assumption of independence. A larger chi-square statistic suggests a greater difference between the observed and expected frequencies.\n",
    "\n",
    "- **Degree of freedom** <br>\n",
    "Represent the number of categories in the variables minus 1. In the context of a chi-square test, it determines the critical values or the distribution of the chi-square statistic. The degrees of freedom help in assessing the statistical significance of the chi-square test.\n",
    "\n",
    "- **P-value** <br>\n",
    "Indicates the statistical significance of the association, if the p-value is below a chosen significance level (alpha < 0.05), it suggests a significant association."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through analysis of the results, we can conclude that, for all the variables analysed, in none of them the p-value is higher or equal to the chosen significance level (alpha = 0.05), only lower. What we can perceive from the fact that the p-value is zero (or near zero) is that the observed results are highly unlikely to be due to random chance, and there is a significant relationship or effect present in the data. As a result, we can only take conclusions based on the chi-square Statistic and the degrees of freedom of each variable. Normally, the higher the degree of freedom the higher the chi-square statistic tends to be. By observing the table, we can understand that some variables have stronger relation with the target variable, such as \"device_os\", \"housing_status\", \"email_is_free\", \"phone_home_valid\", \"has_other_cards\" and \"keep_alive_session\"."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering and Selection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Encoding\n",
    "In order to encode some of our variables, we looked up some of the most used encoding methods. We ended up choosing Ordinal Encoding for High-Cardinality Variables because we have categorical variables with a high number of unique values (high-cardinality). Being that using One-Hot Encoding might lead to a large number of resulting columns and given that we want to reduce the number of columns, One-Hot Encoding turns out not being the best choice. So we decided to use Ordinal Encoding, which assigns unique integers to each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = df.copy()\n",
    "\n",
    "columns_to_encode = ['payment_type', 'employment_status', 'housing_status', 'source', 'device_os']\n",
    "\n",
    "encoder = OrdinalEncoder()\n",
    "\n",
    "dataset[columns_to_encode] = encoder.fit_transform(dataset[columns_to_encode])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Normalization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Explain why we need to normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_dataset = dataset.copy()\n",
    "\n",
    "# Select the columns to normalize (excluding the target variable)\n",
    "columns_to_normalize = dataset.columns.drop(target_col)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler on the selected columns\n",
    "scaler.fit(normalized_dataset[columns_to_normalize])\n",
    "\n",
    "# Transform the selected columns using the scaler\n",
    "normalized_dataset[columns_to_normalize] = scaler.transform(normalized_dataset[columns_to_normalize])\n",
    "\n",
    "dataset = normalized_dataset.copy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset is 1 Million lines long, so we decided to use only 10% of the dataset to work with, in order to reduce the time it takes to run the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_class = dataset[dataset[target_col] == 1]\n",
    "not_fraud_class = dataset[dataset[target_col] == 0]\n",
    "\n",
    "fraud_samples = fraud_class.sample(frac=0.1, random_state=1)\n",
    "not_fraud_samples = not_fraud_class.sample(frac=0.1, random_state=1)\n",
    "\n",
    "dataset = pd.concat([fraud_samples, not_fraud_samples])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting relevant features\n",
    "In this step we select the features that have the most impact on our target variable (\"fraud_bool\"). "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After usage of various statistical techniques and respective analysis, as shown in the steps above, the features that we perceived as being of more relevance were the following: \n",
    "- \"income\"\n",
    "- \"date_of_birth_distinct_emails_4w\"\n",
    "- \"credit_risk_score\"\n",
    "- \"device_os\"\n",
    "- \"housing_status\"\n",
    "- \"email_is_free\"\n",
    "- \"phone_home_valid\"\n",
    "- \"has_other_cards\"\n",
    "- \"keep_alive_session\"\n",
    "- \"velocity_6h\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_features = [\"fraud_bool\", \"income\", \"date_of_birth_distinct_emails_4w\", \"credit_risk_score\", \"device_os\", \n",
    "                     \"housing_status\", \"email_is_free\", \"phone_home_valid\", \"has_other_cards\", \"keep_alive_session\", \n",
    "                     \"velocity_6h\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imbalanced Dataset\n",
    "#### Undersampling vs Oversampling vs SMOTE\n",
    "Dealing with imbalanced datasets is an important aspect of machine learning. In this section, we will explore some techniques for dealing with imbalanced datasets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(axes, report, cm, info):\n",
    "    ax = axes\n",
    "    ax.text(0.4, -.6, report, transform=ax.transAxes, fontsize=10, ha='center')\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Reds', cbar=False, ax=ax)\n",
    "    ax.set_xlabel('Predicted Label')\n",
    "    ax.set_ylabel('True Label')\n",
    "    ax.set_title(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_info = [\"Original Dataset\", \"Under-sampled Dataset\", \"Over-sampled Dataset\", \"SMOTE Dataset\"]\n",
    "algorithm = [RandomForestClassifier(random_state=1), LogisticRegression(random_state=1, max_iter= 1000), DecisionTreeClassifier(random_state=1)]\n",
    "\n",
    "training_data = dataset.copy()\n",
    "\n",
    "# Separate the features and the target variable\n",
    "X = training_data.drop(target_col, axis=1)\n",
    "y = training_data[target_col]\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1, stratify=y)\n",
    "\n",
    "\n",
    "# Shuffle the training data\n",
    "X_train, y_train = shuffle(X_train, y_train, random_state=1)\n",
    "\n",
    "# Train various classifiers\n",
    "num_cols = 3\n",
    "num_rows = (len(algorithm) + num_cols - 1) // num_cols\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, 8))\n",
    "\n",
    "for i, clf in enumerate(algorithm):\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    report = classification_report(y_test, y_pred, zero_division=0)\n",
    "\n",
    "    # Plot the confusion matrix\n",
    "    plot_confusion_matrix(axes[i], report, cm, dataset_info[i] + \" - \" + str(clf).split('(')[0])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x=y_train)\n",
    "plt.title('Target Variable Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original dataset without any techniques applied to deal with the imbalanced dataset results in a model that is biased towards predicting non-fraud cases. As we can see from the confusion matrixes, the models are not able to predict fraud cases, which is the main objective of this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Apply Random Under-sampling to balance the data\n",
    "rus = RandomUnderSampler(random_state=1)\n",
    "X_resampled_rus, y_resampled_rus = rus.fit_resample(X_train, y_train)\n",
    "\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, 8))\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "predicted_us = []\n",
    "for i, clf in enumerate(algorithm):\n",
    "    clf.fit(X_resampled_rus, y_resampled_rus)\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    y_pred = clf.predict(X_test)\n",
    "    predicted_us.append(y_pred)\n",
    "\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    report = classification_report(y_test, y_pred, zero_division=0)\n",
    "\n",
    "    # Plot the confusion matrix\n",
    "    plot_confusion_matrix(axes[i], report, cm, dataset_info[i] + \" - \" + str(clf).split('(')[0])\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x=y_resampled_rus)\n",
    "plt.title('Target Variable Distribution on y_resampled_rus')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplying a undersampling technique, we can see that the target variable is now balanced. Analyzing the confusion matrixes of the different classifiers, we infer that the model is now predicting some fraud cases correctly, but it is also predicting a lot of non-fraud cases as fraud. This is not ideal, because we want to predict fraud cases, but we also want to minimize the number of non-fraud cases that are predicted as fraud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Random Over-sampling to balance the data\n",
    "ros = RandomOverSampler(random_state=1)\n",
    "X_resampled_ros, y_resampled_ros = ros.fit_resample(X_train, y_train)\n",
    "\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, 7))\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "predicted_os = []\n",
    "for i, clf in enumerate(algorithm):\n",
    "    clf.fit(X_resampled_ros, y_resampled_ros)\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    y_pred = clf.predict(X_test)\n",
    "    predicted_os.append(y_pred)\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    report = classification_report(y_test, y_pred, zero_division=0)\n",
    "\n",
    "    # Plot the confusion matrix\n",
    "    plot_confusion_matrix(axes[i], report, cm, dataset_info[i] + \" - \" + str(clf).split('(')[0])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x=y_resampled_ros)\n",
    "plt.title('Target Variable Distribution on y_resampled_ros')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplying a oversampling technique, randomly duplicates instances of the minority class until the dataset is balanced as we can tell by the target variable distribution.\n",
    "By observing the confusion matrixes, the results are very different from the classifiers, so it is difficult to draw conclusions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply SMOTE to balance the data\n",
    "smote = SMOTE(random_state=1)\n",
    "X_resampled_smote, y_resampled_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, 7))\n",
    "\n",
    "predicted_smote = []\n",
    "# Evaluate the model on the test set\n",
    "\n",
    "for i, clf in enumerate(algorithm):\n",
    "    clf.fit(X_resampled_smote, y_resampled_smote)\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    y_pred = clf.predict(X_test)\n",
    "    predicted_smote.append(y_pred)\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    report = classification_report(y_test, y_pred, zero_division=0)\n",
    "\n",
    "    # Plot the confusion matrix\n",
    "    plot_confusion_matrix(axes[i], report, cm, dataset_info[i] + \" - \" + str(clf).split('(')[0])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x=y_resampled_smote)\n",
    "plt.title('Target Variable Distribution on y_resampled_smote')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Concerning the Smote technique, although the improvements were relatively small we noticed a slight improvement regarding the original dataset.\n",
    "The results are similar to the previous technique, as the smote is a oversampling technique which creates synthetic instances of the minority class.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Roc Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the false positive rate (FPR), true positive rate (TPR), and threshold\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "\n",
    "for i in range(len(algorithm)):\n",
    "    # Undersampling\n",
    "    fpr_undersampling, tpr_undersampling, thresholds = roc_curve(y_test, predicted_us[i])\n",
    "    auc_undersampling = auc(fpr_undersampling, tpr_undersampling)\n",
    "    axes[i].set_title(f'{algorithm[i].__class__.__name__}')\n",
    "    axes[i].plot(fpr_undersampling, tpr_undersampling, color='red', lw=2, label='ROC curve - Undersampling (area = %0.2f)' % auc_undersampling)\n",
    "    axes[i].set_xlabel('False Positive Rate')\n",
    "    axes[i].set_ylabel('True Positive Rate')\n",
    "\n",
    "\n",
    "    # SMOTE\n",
    "    fpr_smote, tpr_smote, thresholds = roc_curve(y_test, predicted_smote[i])\n",
    "    auc_smote = auc(fpr_smote, tpr_smote)\n",
    "    axes[i].plot(fpr_smote, tpr_smote, color='green', lw=2, label='ROC curve - SMOTE (area = %0.2f)' % auc_smote)\n",
    "\n",
    "\n",
    "\n",
    "    # Oversampling\n",
    "    fpr_oversampling, tpr_oversampling, thresholds = roc_curve(y_test, predicted_os[i])\n",
    "    auc_oversampling = auc(fpr_oversampling, tpr_oversampling)\n",
    "    axes[i].plot(fpr_oversampling, tpr_oversampling, color='blue', lw=2, label='ROC curve - Oversampling (area = %0.2f)' % auc_oversampling)\n",
    "\n",
    "\n",
    "    axes[i].legend()\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ROC curve is useful for visualizing the performance of a binary classifier. It plots the true positive rate (TPR) against the false positive rate (FPR) for different threshold values. The area under the curve (AUC) is a great measure to compare different models.\n",
    "As we can see from the ROC curves, the technique that appears to deal better with this specific imbalanced dataset is the undersampling technique, as it has the highest AUC score throughout all the classifiers."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Takeaways from Analysis\n",
    "\n",
    "TODO: Add more details about the main takeaways from analysis and preprocessing phase"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, the results confirm that the original dataset suffers from severe class imbalance, leading to poor performance in identifying fraudulent cases. While the applied techniques (under-sampling, over-sampling, and SMOTE) show some improvements in identifying fraudulent cases compared to the original dataset, the performance remains limited. We will need to further analyse and experiment to develop a  more effective model for fraud detection in this dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets preparation for the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of dataframes we will disponibilize throughout the notebook\n",
    "\n",
    "df_all_features = -1\n",
    "df_most_correlated_features = -1\n",
    "df_least_correlated_features = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_train_test(dataset, target_col):\n",
    "    # Separate the features and the target variable\n",
    "    X = dataset.drop(target_col, axis=1)\n",
    "    Y = dataset[target_col]\n",
    "\n",
    "    # Split the data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=1, stratify=Y)\n",
    "\n",
    "    # Shuffle the training data\n",
    "    X_train, Y_train = shuffle(X_train, y_train, random_state=1)\n",
    "\n",
    "    return X_train, X_test, Y_train, y_test\n",
    "\n",
    "\n",
    "undersample = RandomUnderSampler(random_state=1)\n",
    "\n",
    "\n",
    "# all features\n",
    "df_all_features = dataset.copy()\n",
    "X_train_all_features, X_test_all_features, Y_train_all_features, Y_test_all_features = separate_train_test(df_all_features, target_col)\n",
    "df_all_features_train = pd.concat([X_train_all_features, Y_train_all_features], axis=1)\n",
    "df_all_features_test = pd.concat([X_test_all_features, Y_test_all_features], axis=1)\n",
    "\n",
    "# all features undersampled\n",
    "X_train_all_features_under, Y_train_all_features_under = undersample.fit_resample(X_train_all_features, Y_train_all_features)\n",
    "df_all_features_undersampled_train = pd.concat([X_train_all_features_under, Y_train_all_features_under], axis=1)\n",
    "df_all_features_undersampled_test = pd.concat([X_test_all_features, Y_test_all_features], axis=1)\n",
    "\n",
    "\n",
    "# least correlated features\n",
    "least_correlated = dataset.columns[~dataset.columns.isin(relevant_features[1:])]\n",
    "df_least_correlated_features = dataset[least_correlated].copy()\n",
    "X_train_least_correlated, X_test_least_correlated, Y_train_least_correlated, Y_test_least_correlated = separate_train_test(df_least_correlated_features, target_col)\n",
    "df_least_correlated_features_train = pd.concat([X_train_least_correlated, Y_train_least_correlated], axis=1)\n",
    "df_least_correlated_features_test = pd.concat([X_test_least_correlated, Y_test_least_correlated], axis=1)\n",
    "\n",
    "# least correlated features undersampled\n",
    "X_train_least_correlated_under, Y_train_least_correlated_under = undersample.fit_resample(X_train_least_correlated, Y_train_least_correlated)\n",
    "df_least_correlated_features_undersampled_train = pd.concat([X_train_least_correlated_under, Y_train_least_correlated_under], axis=1)\n",
    "df_least_correlated_features_undersampled_test = pd.concat([X_test_least_correlated, Y_test_least_correlated], axis=1)\n",
    "\n",
    "\n",
    "# most correlated features\n",
    "df_most_correlated_features = dataset[relevant_features].copy()\n",
    "X_train_most_correlated, X_test_most_correlated, Y_train_most_correlated, Y_test_most_correlated = separate_train_test(df_most_correlated_features, target_col)\n",
    "df_most_correlated_features_train = pd.concat([X_train_most_correlated, Y_train_most_correlated], axis=1)\n",
    "df_most_correlated_features_test = pd.concat([X_test_most_correlated, Y_test_most_correlated], axis=1)\n",
    "\n",
    "# most correlated features undersampled\n",
    "X_train_most_correlated_under, Y_train_most_correlated_under = undersample.fit_resample(X_train_most_correlated, Y_train_most_correlated)\n",
    "df_most_correlated_features_undersampled_train = pd.concat([X_train_most_correlated_under, Y_train_most_correlated_under], axis=1)\n",
    "df_most_correlated_features_undersampled_test = pd.concat([X_test_most_correlated, Y_test_most_correlated], axis=1)\n",
    "\n",
    "\n",
    "df_all_features_undersampled = {\"train\": df_all_features_undersampled_train, \"test\": df_all_features_undersampled_test}\n",
    "df_least_correlated_features_undersampled = {\"train\": df_least_correlated_features_undersampled_train, \"test\": df_least_correlated_features_undersampled_test}\n",
    "df_most_correlated_features_undersampled = {\"train\": df_most_correlated_features_undersampled_train, \"test\": df_most_correlated_features_undersampled_test}\n",
    "\n",
    "\n",
    "dataframes = [df_all_features_undersampled, df_least_correlated_features_undersampled, df_most_correlated_features_undersampled]\n",
    "dataframes_names = [\"all_features\", \"least_correlated_features\", \"most_correlated_features\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Modeling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing modeling techniques\n",
    "The modeling techniques chosen are: \n",
    "- Naive Bayes\n",
    "- Decision Tree\n",
    "- k-Nearest Neighbours (KNN)\n",
    "- Logistic Regression\n",
    "- Support Vectors (SVM)\n",
    "- Random Forest"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying models and evaluating performance\n",
    "In this section, we apply the models chosen and evaluate their performance with appropriate metrics.\n",
    "\n",
    "The metrics chosen are the following:\n",
    "- Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes\n",
    "Naive Bayes is a classification algorithm based on Bayes' theorem. It assumes that features are independent and calculates the probability of an instance belonging to a class. It's computationally efficient, works well with high-dimensional data, and performs best when the independence assumption holds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Naive Bayes classifier\n",
    "nb = GaussianNB()\n",
    "\n",
    "num_cols = 3\n",
    "num_rows = 1\n",
    "\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, 8))\n",
    "prediction_nb = []\n",
    "\n",
    "for i, (df, name) in enumerate(zip(dataframes, dataframes_names)):\n",
    "    # Fit the Naive Bayes classifier\n",
    "    nb.fit(df[\"train\"].drop(target_col, axis=1), df[\"train\"][target_col])\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred = nb.predict(df[\"test\"].drop(target_col, axis=1))\n",
    "    prediction_nb.append(y_pred)\n",
    "\n",
    "    # Calculate the confusion matrix and classification report\n",
    "    cm = confusion_matrix(df[\"test\"][target_col], y_pred)\n",
    "    report = classification_report(df[\"test\"][target_col], y_pred, zero_division=0)\n",
    "\n",
    "    # Plot the confusion matrix\n",
    "    plot_confusion_matrix(axes[i], report, cm, \"Naive_Bayes - \" + name)\n",
    "\n",
    "# Remove empty subplots if the number of plots is not a multiple of num_cols\n",
    "if len(dataframes) % num_cols != 0:\n",
    "    for i in range(len(dataframes) % num_cols, num_cols):\n",
    "        fig.delaxes(axes[i % num_rows, i // num_rows])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree\n",
    "Decision Trees are classification algorithms that create a tree-like model of decisions. Each internal node represents a feature, and each leaf node represents a class label. They split the data based on feature values to create homogeneous subsets. When making predictions, a new instance traverses the tree to a leaf node, and the corresponding class label is assigned. Decision Trees are interpretable and handle categorical and numerical data, capturing complex decision boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Decision Tree classifier\n",
    "dt = DecisionTreeClassifier(random_state=1)\n",
    "\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, 8))\n",
    "prediction_dt = []\n",
    "\n",
    "for i, (df, name) in enumerate(zip(dataframes, dataframes_names)):\n",
    "    # Fit the Decision Tree classifier\n",
    "    dt.fit(df[\"train\"].drop(target_col, axis=1), df[\"train\"][target_col])\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred = dt.predict(df[\"test\"].drop(target_col, axis=1))\n",
    "    prediction_dt.append(y_pred)\n",
    "\n",
    "    # Calculate the confusion matrix and classification report\n",
    "    cm = confusion_matrix(df[\"test\"][target_col], y_pred)\n",
    "    report = classification_report(df[\"test\"][target_col], y_pred, zero_division=0)\n",
    "\n",
    "    # Plot the confusion matrix\n",
    "    plot_confusion_matrix(axes[i], report, cm, \"Decision_Tree - \" + name)\n",
    "\n",
    "# Remove empty subplots if the number of plots is not a multiple of num_cols\n",
    "if len(dataframes) % num_cols != 0:\n",
    "    for i in range(len(dataframes) % num_cols, num_cols):\n",
    "        fig.delaxes(axes[i % num_rows, i // num_rows])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### k-Nearest Neighbours (KNN) \n",
    "k-Nearest Neighbors (KNN) is a classification algorithm that predicts the class of an instance based on its k nearest neighbors in the feature space. It assumes that instances with similar features tend to belong to the same class. During training, KNN stores the feature vectors and their corresponding class labels. When making predictions, it finds the k nearest neighbors to the target instance and assigns the majority class among those neighbors as the predicted class. KNN is a simple and versatile algorithm that can handle both linear and non-linear classification tasks, making it useful in various applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a KNN model\n",
    "model = KNeighborsClassifier()\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, 8))\n",
    "prediction_knn = []\n",
    "\n",
    "for i, (df, name) in enumerate(zip(dataframes, dataframes_names)):\n",
    "    # Fit the KNN model\n",
    "    model.fit(df[\"train\"].drop(target_col, axis=1), df[\"train\"][target_col])\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred = model.predict(df[\"test\"].drop(target_col, axis=1))\n",
    "    prediction_knn.append(y_pred)\n",
    "\n",
    "    # Calculate the confusion matrix and classification report\n",
    "    cm = confusion_matrix(df[\"test\"][target_col], y_pred)\n",
    "    report = classification_report(df[\"test\"][target_col], y_pred, zero_division=0)\n",
    "\n",
    "    # Plot the confusion matrix\n",
    "    plot_confusion_matrix(axes[i], report, cm, \"KNN - \" + name)\n",
    "\n",
    "# Remove empty subplots if the number of plots is not a multiple of num_cols\n",
    "if len(dataframes) % num_cols != 0:\n",
    "    for i in range(len(dataframes) % num_cols, num_cols):\n",
    "        fig.delaxes(axes[i % num_rows, i // num_rows])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression\n",
    "Logistic Regression is a classification algorithm that predicts the probability of an instance belonging to a specific class. It uses a sigmoid function to map input features to a binary output. By fitting a decision boundary during training, Logistic Regression separates the classes. When making predictions, it calculates the probability of an instance belonging to the positive class and applies a threshold for classification. Logistic Regression is a simple and effective algorithm suitable for binary classification tasks, handling linearly and non-linearly separable data with appropriate transformations or kernel functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "model = LogisticRegression(random_state=1, max_iter= 1000)\n",
    "\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, 8))\n",
    "prediction_lr = []\n",
    "\n",
    "for i, (df, name) in enumerate(zip(dataframes, dataframes_names)):\n",
    "    # Fit the Logistic Regression model\n",
    "    model.fit(df[\"train\"].drop(target_col, axis=1), df[\"train\"][target_col])\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred = model.predict(df[\"test\"].drop(target_col, axis=1))\n",
    "    prediction_lr.append(y_pred)\n",
    "\n",
    "    # Calculate the confusion matrix and classification report\n",
    "    cm = confusion_matrix(df[\"test\"][target_col], y_pred)\n",
    "    report = classification_report(df[\"test\"][target_col], y_pred, zero_division=0)\n",
    "\n",
    "    # Plot the confusion matrix\n",
    "    plot_confusion_matrix(axes[i], report, cm, \"Logistic_Regression - \" + name)\n",
    "\n",
    "# Remove empty subplots if the number of plots is not a multiple of num_cols\n",
    "if len(dataframes) % num_cols != 0:\n",
    "    for i in range(len(dataframes) % num_cols, num_cols):\n",
    "        fig.delaxes(axes[i % num_rows, i // num_rows])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vectors (SVM)\n",
    "Support Vector Machines (SVM) are powerful classifiers that can handle both linear and non-linear classification tasks. They work by finding an optimal hyperplane that maximally separates the data points of different classes. SVMs also offer various kernels (e.g., linear, polynomial, radial basis function) to capture complex relationships between the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support Vector Machine\n",
    "model = SVC(random_state=1)\n",
    "\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, 8))\n",
    "prediction_svm = []\n",
    "\n",
    "for i, (df, name) in enumerate(zip(dataframes, dataframes_names)):\n",
    "    # Fit the SVM model\n",
    "    model.fit(df[\"train\"].drop(target_col, axis=1), df[\"train\"][target_col])\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred = model.predict(df[\"test\"].drop(target_col, axis=1))\n",
    "    prediction_svm.append(y_pred)\n",
    "\n",
    "    # Calculate the confusion matrix and classification report\n",
    "    cm = confusion_matrix(df[\"test\"][target_col], y_pred)\n",
    "    report = classification_report(df[\"test\"][target_col], y_pred, zero_division=0)\n",
    "\n",
    "    # Plot the confusion matrix\n",
    "    plot_confusion_matrix(axes[i], report, cm, \"SVM - \" + name)\n",
    "\n",
    "# Remove empty subplots if the number of plots is not a multiple of num_cols\n",
    "if len(dataframes) % num_cols != 0:\n",
    "    for i in range(len(dataframes) % num_cols, num_cols):\n",
    "        fig.delaxes(axes[i % num_rows, i // num_rows])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest \n",
    "Random Forest is an ensemble learning method that combines multiple decision trees to make predictions. Each decision tree is trained on a random subset of the data, and the final prediction is determined by aggregating the predictions of individual trees. Random Forests are effective in handling complex datasets, capturing non-linear relationships, and mitigating overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Random Forest Classifier\n",
    "model = RandomForestClassifier(random_state=1)\n",
    "\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, 8))\n",
    "prediction_random_forest = []\n",
    "for i, (df, name) in enumerate(zip(dataframes, dataframes_names)):\n",
    "    # Fit the Random Forest Classifier\n",
    "    model.fit(df[\"train\"].drop(target_col, axis=1), df[\"train\"][target_col])\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred = model.predict(df[\"test\"].drop(target_col, axis=1))\n",
    "    prediction_random_forest.append(y_pred)\n",
    "\n",
    "    # Calculate the confusion matrix and classification report\n",
    "    cm = confusion_matrix(df[\"test\"][target_col], y_pred)\n",
    "    report = classification_report(df[\"test\"][target_col], y_pred, zero_division=0)\n",
    "\n",
    "    # Plot the confusion matrix\n",
    "    plot_confusion_matrix(axes[i], report, cm, \"Random_Forest - \" + name)\n",
    "\n",
    "# Remove empty subplots if the number of plots is not a multiple of num_cols\n",
    "if len(dataframes) % num_cols != 0:\n",
    "    for i in range(len(dataframes) % num_cols, num_cols):\n",
    "        fig.delaxes(axes[i % num_rows, i // num_rows])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "algorithms = ['Naive Bayes', 'Decision Tree', 'KNN', 'Logistic Regression', 'SVM', 'Random Forest']\n",
    "# Compute the false positive rate (FPR), true positive rate (TPR), and threshold\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "#Naive Bayes\n",
    "fpr_all_features, tpr_all_features, thresholds = roc_curve(y_test, prediction_nb[0])\n",
    "auc_all_features = auc(fpr_all_features, tpr_all_features)\n",
    "axes[0].set_title(f'{algorithms[0]}')\n",
    "axes[0].plot(fpr_all_features, tpr_all_features, color='red', lw=2, label='ROC curve - All Features (area = %0.2f)' % auc_all_features)\n",
    "axes[0].set_xlabel('False Positive Rate')\n",
    "axes[0].set_ylabel('True Positive Rate')\n",
    "\n",
    "fpr_least_correlated_features, tpr_least_correlated_features, thresholds = roc_curve(y_test, prediction_nb[1])\n",
    "auc_least_correlated_features = auc(fpr_least_correlated_features, tpr_least_correlated_features)\n",
    "axes[0].plot(fpr_least_correlated_features, tpr_least_correlated_features, color='green', lw=2, label='ROC curve - Least Correlated Features (area = %0.2f)' % auc_least_correlated_features)\n",
    "\n",
    "fpr_most_correlated_features, tpr_most_correlated_features, thresholds = roc_curve(y_test, prediction_nb[2])\n",
    "auc_most_correlated_features = auc(fpr_most_correlated_features, tpr_most_correlated_features)\n",
    "axes[0].plot(fpr_most_correlated_features, tpr_most_correlated_features, color='blue', lw=2, label='ROC curve - Most Correlated Features (area = %0.2f)' % auc_most_correlated_features)\n",
    "\n",
    "axes[0].legend()\n",
    "\n",
    "#Decision Tree\n",
    "fpr_all_features, tpr_all_features, thresholds = roc_curve(y_test, prediction_dt[0])\n",
    "auc_all_features = auc(fpr_all_features, tpr_all_features)\n",
    "axes[1].set_title(f'{algorithms[1]}')\n",
    "axes[1].plot(fpr_all_features, tpr_all_features, color='red', lw=2, label='ROC curve - All Features (area = %0.2f)' % auc_all_features)\n",
    "axes[1].set_xlabel('False Positive Rate')\n",
    "axes[1].set_ylabel('True Positive Rate')\n",
    "\n",
    "fpr_least_correlated_features, tpr_least_correlated_features, thresholds = roc_curve(y_test, prediction_dt[1])\n",
    "auc_least_correlated_features = auc(fpr_least_correlated_features, tpr_least_correlated_features)\n",
    "axes[1].plot(fpr_least_correlated_features, tpr_least_correlated_features, color='green', lw=2, label='ROC curve - Least Correlated Features (area = %0.2f)' % auc_least_correlated_features)\n",
    "\n",
    "fpr_most_correlated_features, tpr_most_correlated_features, thresholds = roc_curve(y_test, prediction_dt[2])\n",
    "auc_most_correlated_features = auc(fpr_most_correlated_features, tpr_most_correlated_features)\n",
    "axes[1].plot(fpr_most_correlated_features, tpr_most_correlated_features, color='blue', lw=2, label='ROC curve - Most Correlated Features (area = %0.2f)' % auc_most_correlated_features)\n",
    "\n",
    "axes[1].legend()\n",
    "\n",
    "#KNN\n",
    "fpr_all_features, tpr_all_features, thresholds = roc_curve(y_test, prediction_knn[0])\n",
    "auc_all_features = auc(fpr_all_features, tpr_all_features)\n",
    "axes[2].set_title(f'{algorithms[2]}')\n",
    "axes[2].plot(fpr_all_features, tpr_all_features, color='red', lw=2, label='ROC curve - All Features (area = %0.2f)' % auc_all_features)\n",
    "axes[2].set_xlabel('False Positive Rate')\n",
    "axes[2].set_ylabel('True Positive Rate')\n",
    "\n",
    "fpr_least_correlated_features, tpr_least_correlated_features, thresholds = roc_curve(y_test, prediction_knn[1])\n",
    "auc_least_correlated_features = auc(fpr_least_correlated_features, tpr_least_correlated_features)\n",
    "axes[2].plot(fpr_least_correlated_features, tpr_least_correlated_features, color='green', lw=2, label='ROC curve - Least Correlated Features (area = %0.2f)' % auc_least_correlated_features)\n",
    "\n",
    "fpr_most_correlated_features, tpr_most_correlated_features, thresholds = roc_curve(y_test, prediction_knn[2])\n",
    "auc_most_correlated_features = auc(fpr_most_correlated_features, tpr_most_correlated_features)\n",
    "\n",
    "axes[2].plot(fpr_most_correlated_features, tpr_most_correlated_features, color='blue', lw=2, label='ROC curve - Most Correlated Features (area = %0.2f)' % auc_most_correlated_features)\n",
    "\n",
    "axes[2].legend()\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Logistic Regression\n",
    "fpr_all_features, tpr_all_features, thresholds = roc_curve(y_test, prediction_lr[0])\n",
    "auc_all_features = auc(fpr_all_features, tpr_all_features)\n",
    "axes[0].set_title(f'{algorithms[3]}')\n",
    "axes[0].plot(fpr_all_features, tpr_all_features, color='red', lw=2, label='ROC curve - All Features (area = %0.2f)' % auc_all_features)\n",
    "axes[0].set_xlabel('False Positive Rate')\n",
    "axes[0].set_ylabel('True Positive Rate')\n",
    "\n",
    "fpr_least_correlated_features, tpr_least_correlated_features, thresholds = roc_curve(y_test, prediction_lr[1])\n",
    "auc_least_correlated_features = auc(fpr_least_correlated_features, tpr_least_correlated_features)\n",
    "axes[0].plot(fpr_least_correlated_features, tpr_least_correlated_features, color='green', lw=2, label='ROC curve - Least Correlated Features (area = %0.2f)' % auc_least_correlated_features)\n",
    "\n",
    "fpr_most_correlated_features, tpr_most_correlated_features, thresholds = roc_curve(y_test, prediction_lr[2])\n",
    "auc_most_correlated_features = auc(fpr_most_correlated_features, tpr_most_correlated_features)\n",
    "axes[0].plot(fpr_most_correlated_features, tpr_most_correlated_features, color='blue', lw=2, label='ROC curve - Most Correlated Features (area = %0.2f)' % auc_most_correlated_features)\n",
    "\n",
    "axes[0].legend()\n",
    "\n",
    "# SVM\n",
    "fpr_all_features, tpr_all_features, thresholds = roc_curve(y_test, prediction_svm[0])\n",
    "auc_all_features = auc(fpr_all_features, tpr_all_features)\n",
    "axes[1].set_title(f'{algorithms[4]}')\n",
    "axes[1].plot(fpr_all_features, tpr_all_features, color='red', lw=2, label='ROC curve - All Features (area = %0.2f)' % auc_all_features)\n",
    "axes[1].set_xlabel('False Positive Rate')\n",
    "axes[1].set_ylabel('True Positive Rate')\n",
    "\n",
    "fpr_least_correlated_features, tpr_least_correlated_features, thresholds = roc_curve(y_test, prediction_svm[1])\n",
    "auc_least_correlated_features = auc(fpr_least_correlated_features, tpr_least_correlated_features)\n",
    "axes[1].plot(fpr_least_correlated_features, tpr_least_correlated_features, color='green', lw=2, label='ROC curve - Least Correlated Features (area = %0.2f)' % auc_least_correlated_features)\n",
    "\n",
    "fpr_most_correlated_features, tpr_most_correlated_features, thresholds = roc_curve(y_test, prediction_svm[2])\n",
    "auc_most_correlated_features = auc(fpr_most_correlated_features, tpr_most_correlated_features)\n",
    "axes[1].plot(fpr_most_correlated_features, tpr_most_correlated_features, color='blue', lw=2, label='ROC curve - Most Correlated Features (area = %0.2f)' % auc_most_correlated_features)\n",
    "\n",
    "axes[1].legend()\n",
    "\n",
    "# Random Forest\n",
    "fpr_all_features, tpr_all_features, thresholds = roc_curve(y_test, prediction_random_forest[0])\n",
    "auc_all_features = auc(fpr_all_features, tpr_all_features)\n",
    "axes[2].set_title(f'{algorithms[5]}')\n",
    "axes[2].plot(fpr_all_features, tpr_all_features, color='red', lw=2, label='ROC curve - All Features (area = %0.2f)' % auc_all_features)\n",
    "axes[2].set_xlabel('False Positive Rate')\n",
    "axes[2].set_ylabel('True Positive Rate')\n",
    "\n",
    "fpr_least_correlated_features, tpr_least_correlated_features, thresholds = roc_curve(y_test, prediction_random_forest[1])\n",
    "auc_least_correlated_features = auc(fpr_least_correlated_features, tpr_least_correlated_features)\n",
    "axes[2].plot(fpr_least_correlated_features, tpr_least_correlated_features, color='green', lw=2, label='ROC curve - Least Correlated Features (area = %0.2f)' % auc_least_correlated_features)\n",
    "\n",
    "fpr_most_correlated_features, tpr_most_correlated_features, thresholds = roc_curve(y_test, prediction_random_forest[2])\n",
    "auc_most_correlated_features = auc(fpr_most_correlated_features, tpr_most_correlated_features)\n",
    "axes[2].plot(fpr_most_correlated_features, tpr_most_correlated_features, color='blue', lw=2, label='ROC curve - Most Correlated Features (area = %0.2f)' % auc_most_correlated_features)\n",
    "\n",
    "axes[2].legend()\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this ROC curves and the recall scores, we can see all the algorithms have a similar performance on all datasets, except for the dataset with only the least relevant features, where the algorithms have a lower performance. The algorithms with the best performance are the Logistic Regression the Support Vectors and Random Forest, with the Logistic Regression having a slightly better performance."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models Evaluation and Validation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying cross-validation techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the recall scorer with average parameter\n",
    "recall_scorer = make_scorer(recall_score, average='binary')\n",
    "\n",
    "# Create the Logistic Regression model\n",
    "model = LogisticRegression(C=0.1, solver='liblinear', max_iter=100)\n",
    "\n",
    "# Define the number of folds\n",
    "k = 5\n",
    "\n",
    "# Create a KFold object with shuffle=True\n",
    "kf = KFold(n_splits=k, shuffle=True)\n",
    "\n",
    "# Perform cross-validation using the recall scorer\n",
    "scores = cross_val_score(model, df_most_correlated_features_undersampled_train.drop(target_col, axis=1), df_most_correlated_features_undersampled_train[target_col], cv=kf, scoring=recall_scorer)\n",
    "\n",
    "# Print the average score\n",
    "print(\"Average Recall:\", scores.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(df_most_correlated_features_undersampled_train.drop(target_col, axis=1), df_most_correlated_features_undersampled_train[target_col])\n",
    "\n",
    "y_pred = model.predict(df_most_correlated_features_undersampled_test.drop(target_col, axis=1))\n",
    "\n",
    "# Calculate the confusion matrix and classification report\n",
    "cm = confusion_matrix(df_most_correlated_features_undersampled_test[target_col], y_pred)\n",
    "\n",
    "report = classification_report(df_most_correlated_features_undersampled_test[target_col], y_pred, zero_division=0)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "fig, axes = plt.subplots(1, 1, figsize=(15, 8))\n",
    "\n",
    "plot_confusion_matrix(axes, report, cm, \"Logistic Regression - Most Correlated Features\", )\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretation and Insights"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After analysing all data collected from ROC plots, we concluded that the algorithm that had the better performance based on AUC (Area Under Curve) was Logistic Regression. \n",
    "\n",
    "Overall, there was not one model that stood out, but we consider the results as being acceptable. However, it is important to highlight that we had preference towards a lower rate of false negatives, as we consider it better given the problem we have in hands. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
